{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the cv2 lib\n",
    "import cv2\n",
    "\n",
    "# OS traversal \n",
    "import os \n",
    "\n",
    "# Pytorch \n",
    "import torch\n",
    "\n",
    "# ONNX models \n",
    "import onnx\n",
    "import onnxruntime\n",
    "\n",
    "# Predicting \n",
    "from tqdm import tqdm \n",
    "\n",
    "# Array math \n",
    "import numpy as np \n",
    "\n",
    "# Data wrangling \n",
    "import pandas as pd \n",
    "\n",
    "# Ploting \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration reading \n",
    "import yaml \n",
    "\n",
    "# Typehinting \n",
    "from typing import Tuple\n",
    "\n",
    "# Defining the path to model and video dirs \n",
    "model_dir = os.path.join(os.getcwd(), 'models')\n",
    "video_dir = os.path.join(os.getcwd(), 'videos')\n",
    "\n",
    "# Defining the directory for extracted images \n",
    "extracted_images_dir = os.path.join(os.getcwd(), 'extracted_images')\n",
    "if not os.path.exists(extracted_images_dir):\n",
    "    os.mkdir(extracted_images_dir)\n",
    "\n",
    "# Postprocessed images \n",
    "postprocessed_images_dir = os.path.join(os.getcwd(), 'postprocessed_images')\n",
    "if not os.path.exists(postprocessed_images_dir):\n",
    "    os.mkdir(postprocessed_images_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the configuration \n",
    "with open('configuration.yml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "\n",
    "# Loading the model name \n",
    "model_name = config['MODEL_TO_USE_WALDO']\n",
    "\n",
    "# Loading the video to blurr \n",
    "video_name = config['VIDEO_TO_USE']\n",
    "\n",
    "# Defining the paths to the model and video\n",
    "model_path = os.path.join(model_dir, model_name)\n",
    "video_path = os.path.join(video_dir, video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_img_size_from_name(x: str) -> Tuple: \n",
    "    \"\"\"\n",
    "    Infers the size of the image from the square model name \n",
    "    \"\"\"\n",
    "    # Spliting by - \n",
    "    x = x.split('-')\n",
    "\n",
    "    # Leaving the entry that has <int>px pattern in it \n",
    "    x = [i for i in x if 'px' in i][0]\n",
    "\n",
    "    # Removing the px\n",
    "    x = x.replace('px', '')\n",
    "\n",
    "    # Converting to int \n",
    "    x = int(x)\n",
    "\n",
    "    # Returning a tuple \n",
    "    return (x, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the image size\n",
    "w, h = infer_img_size_from_name(model_name)\n",
    "\n",
    "# Printing the size \n",
    "print(f'Image size: {w}x{h}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the onnx model \n",
    "model = onnx.load(model_path)\n",
    "\n",
    "# Checking the model \n",
    "onnx.checker.check_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the class dictionary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = config['WALDO_CLASSES']\n",
    "\n",
    "# Creating a classes dictionary \n",
    "classes_dict = dict()\n",
    "for i, c in enumerate(classes):\n",
    "    classes_dict[i] = c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting the steps to do "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = config['STEPS']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the video into images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_img = steps['SPLIT_VIDEO']\n",
    "\n",
    "# Empty placeholder for the split_images_dir \n",
    "split_images_dir = None\n",
    "\n",
    "if split_img:\n",
    "    # Reading the video \n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Saving the number of frames \n",
    "    n_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Creating the path for the split images \n",
    "    split_images_dir = os.path.join(extracted_images_dir, video_name)\n",
    "\n",
    "    # Creating a dir if not exists; Else, removing all the images\n",
    "    if not os.path.exists(split_images_dir):\n",
    "        os.makedirs(split_images_dir, exist_ok=True)\n",
    "    else:\n",
    "        for image in os.listdir(split_images_dir):\n",
    "            os.remove(os.path.join(split_images_dir, image))\n",
    "\n",
    "    # Spliting the video by frame \n",
    "    for i in tqdm(range(n_frames), desc='Spliting the video'):\n",
    "        ret, frame = cap.read()\n",
    "        if ret == False:\n",
    "            break\n",
    "        cv2.imwrite(os.path.join(split_images_dir, f'{str(i)}.jpg'), frame)\n",
    "else:\n",
    "    split_images_dir = os.path.join(extracted_images_dir, video_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blurring the images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the classes to plot \n",
    "classes_to_plot = config['WALDO_CLASSES_PLOT']\n",
    "\n",
    "# Defining the classes to blurr \n",
    "classes_to_blurr = config['WALDO_CLASSES_BLURR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def letterbox(im, new_shape=(960, 960), color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
    "    # Resize and pad image while meeting stride-multiple constraints\n",
    "    shape = im.shape[:2]  # current shape [height, width]\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    # Scale ratio (new / old)\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:  # only scale down, do not scale up (for better val mAP)\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    # Compute padding\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
    "\n",
    "    if auto:  # minimum rectangle\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
    "\n",
    "    dw /= 2  # divide padding into 2 sides\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:  # resize\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
    "    return im, r, (dw, dh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example image pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_images_dir is not None:\n",
    "\n",
    "    # Getting one image path \n",
    "    image_path = os.path.join(split_images_dir, os.listdir(split_images_dir)[0])\n",
    "\n",
    "    # Reading the image from open cv \n",
    "    image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Saving the original image's w and h \n",
    "    original_w, original_h = image.shape[1], image.shape[0]\n",
    "\n",
    "    # Ploting the image \n",
    "    plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping the image to w and h resolution \n",
    "img, ratio, dwdh = letterbox(image, new_shape=(h, w), auto=False, color=(0, 0, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the input for the deep learning model\n",
    "img = img.transpose((2, 0, 1))\n",
    "img = np.expand_dims(img, 0)\n",
    "img = np.ascontiguousarray(img)\n",
    "\n",
    "img = img.astype(np.float32)\n",
    "img /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Starting the session \n",
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Initialize ONNX runtime session with CUDA execution\n",
    "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\n",
    "\n",
    "# Initiating the onnx runtime \n",
    "ort_session = onnxruntime.InferenceSession(model_path, providers=providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting \n",
    "ort_inputs = {ort_session.get_inputs()[0].name: img}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "# Getting the bounding boxes\n",
    "list_of_objects = []\n",
    "if len(ort_outs) > 0:\n",
    "    boxes = ort_outs[0] \n",
    "\n",
    "    print(f\"Number of objects: {len(boxes)}\")\n",
    "\n",
    "    # Iterating over the boxes\n",
    "    for box in boxes:\n",
    "        # The box is a list with the following coordinates: \n",
    "        # (batch_id, x0, y0, x1, y1, cls_id, score)\n",
    "\n",
    "        # Getting the class id\n",
    "        class_id = int(box[5])\n",
    "\n",
    "        # Getting the class name\n",
    "        class_name = classes_dict[class_id]\n",
    "\n",
    "        # Only leaving the classes to plot\n",
    "        if class_name not in classes_to_plot:\n",
    "            continue\n",
    "\n",
    "        # Getting the x, y, w, h\n",
    "        x0, y0, x1, y1 = box[1], box[2], box[3], box[4]\n",
    "\n",
    "        # Transforming the coords \n",
    "        bbox = np.array([x0, y0, x1, y1])\n",
    "        bbox -= np.array(dwdh * 2)\n",
    "        bbox /= ratio\n",
    "        bbox = bbox.round().astype(np.int32).tolist()\n",
    "\n",
    "        # Extracting the prediction score \n",
    "        score = box[6]\n",
    "\n",
    "        # Creating an entry for the list \n",
    "        list_of_objects.append({\n",
    "            'class_id': class_id, \n",
    "            'class_name': class_name, \n",
    "            'x0': bbox[0], \n",
    "            'y0': bbox[1], \n",
    "            'x1': bbox[2], \n",
    "            'y1': bbox[3], \n",
    "            'score': round(float(score), 2)\n",
    "        })\n",
    "\n",
    "print(f\"The number of objects to plot: {len(list_of_objects)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading and plotting the original image\n",
    "img = image = cv2.cvtColor(cv2.imread(image_path), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "# Plotting the bounding boxes\n",
    "for obj in list_of_objects:\n",
    "    # Getting the image shape\n",
    "    h_, w_, _ = img.shape\n",
    "\n",
    "    # Iterating over the rows\n",
    "    for entry in list_of_objects:\n",
    "        # Getting the class name\n",
    "        class_name = entry['class_name']\n",
    "\n",
    "        # Extracting the score\n",
    "        score = entry['score']\n",
    "\n",
    "        # Getting the coordinates\n",
    "        x0, y0, x1, y1 = int(entry['x0']), int(entry['y0']), int(entry['x1']), int(entry['y1'])\n",
    "\n",
    "        # Drawing the rectangle\n",
    "        cv2.rectangle(img, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "\n",
    "        # Putting the class name\n",
    "        cv2.putText(img, f\"{class_name} {score}\", (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "# Plotting the image\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying on all images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting the number of frames to plot \n",
    "n_frames_to_plot = config.get('FRAMES_TO_PLOT', 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda = torch.cuda.is_available()\n",
    "\n",
    "# Initialize ONNX runtime session with CUDA execution\n",
    "providers = ['CUDAExecutionProvider', 'CPUExecutionProvider'] if cuda else ['CPUExecutionProvider']\n",
    "\n",
    "# Initiating the onnx runtime \n",
    "ort_session = onnxruntime.InferenceSession(model_path, providers=providers)\n",
    "\n",
    "# Listing the images in the extracted images dir\n",
    "images = os.listdir(split_images_dir)\n",
    "\n",
    "# Creating the image dictionary where the key is the image name and the image index \n",
    "image_dict = dict()\n",
    "for image in images:\n",
    "    image_dict[image] = int(image.split('.')[0])\n",
    "\n",
    "# Sorting the images by the index\n",
    "images = sorted(images, key=lambda x: image_dict[x])\n",
    "\n",
    "if n_frames_to_plot is not None:\n",
    "    # Getting the first n frames \n",
    "    images = images[:n_frames_to_plot]\n",
    "\n",
    "# Iterating over the images\n",
    "list_of_images = []\n",
    "for image in tqdm(images):\n",
    "    # Predicting the bounding boxes\n",
    "    img = cv2.cvtColor(cv2.imread(os.path.join(split_images_dir, image)), cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    # Preprocesing\n",
    "    img, ratio, dwdh = letterbox(img, new_shape=(h, w), auto=False, color=(0, 0, 0))\n",
    "\n",
    "    # Creating input for the model\n",
    "    img = img.transpose((2, 0, 1))\n",
    "    img = np.expand_dims(img, 0)\n",
    "    img = np.ascontiguousarray(img)\n",
    "\n",
    "    img = img.astype(np.float32)\n",
    "    img /= 255\n",
    "\n",
    "    # Predicting the bounding boxes\n",
    "    ort_inputs = {ort_session.get_inputs()[0].name: img}\n",
    "    ort_outs = ort_session.run(None, ort_inputs)\n",
    "\n",
    "    # Getting the bounding boxes\n",
    "    if len(ort_outs) > 0:\n",
    "        boxes = ort_outs[0] \n",
    "\n",
    "        # Iterating over the boxes\n",
    "        for box in boxes:\n",
    "            # The box is a list with the following coordinates: \n",
    "            # (batch_id, x0, y0, x1, y1, cls_id, score)\n",
    "\n",
    "            # Getting the class id\n",
    "            class_id = int(box[5])\n",
    "\n",
    "            # Getting the class name\n",
    "            class_name = classes_dict[class_id]\n",
    "\n",
    "            # Only leaving the classes to plot\n",
    "            if class_name not in classes_to_plot:\n",
    "                continue\n",
    "\n",
    "            # Getting the x, y, w, h\n",
    "            x0, y0, x1, y1 = box[1], box[2], box[3], box[4]\n",
    "\n",
    "            # Transforming the coords \n",
    "            bbox = np.array([x0, y0, x1, y1])\n",
    "            bbox -= np.array(dwdh * 2)\n",
    "            bbox /= ratio\n",
    "            bbox = bbox.round().astype(np.int32).tolist()\n",
    "\n",
    "            # Extracting the prediction score \n",
    "            score = box[6]\n",
    "\n",
    "            # Creating an entry for the list \n",
    "            list_of_images.append({\n",
    "                'image_path': os.path.join(split_images_dir, image),\n",
    "                'base_image': image,\n",
    "                'class_id': class_id, \n",
    "                'class_name': class_name, \n",
    "                'x0': bbox[0], \n",
    "                'y0': bbox[1], \n",
    "                'x1': bbox[2], \n",
    "                'y1': bbox[3], \n",
    "                'score': round(float(score), 2)\n",
    "            })\n",
    "\n",
    "# Creating an prediction dataframe \n",
    "df = pd.DataFrame(list_of_images)\n",
    "\n",
    "# Sorting by image name \n",
    "df = df.sort_values(by=['image_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drawing the bounding boxes and blurring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the output dir \n",
    "postprocessed_images_dir_video = os.path.join(postprocessed_images_dir, video_name)\n",
    "if not os.path.exists(postprocessed_images_dir_video):\n",
    "    os.mkdir(postprocessed_images_dir_video)\n",
    "\n",
    "# Iterating over the images\n",
    "for image_path in tqdm(df['image_path'].unique()):\n",
    "    # Subsetting the dataframe\n",
    "    df_subset = df[df['image_path'] == image_path].copy()\n",
    "\n",
    "    # Reading the image\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    # Iterating over the rows\n",
    "    for _, row in df_subset.iterrows():\n",
    "        try:\n",
    "            # Getting the class name\n",
    "            class_name = row['class_name']\n",
    "\n",
    "            # Extracting the score\n",
    "            score = row['score']\n",
    "\n",
    "            # Getting the coordinates\n",
    "            x0, y0, x1, y1 = int(row['x0']), int(row['y0']), int(row['x1']), int(row['y1'])\n",
    "\n",
    "            # Drawing the rectangle\n",
    "            cv2.rectangle(img, (x0, y0), (x1, y1), (0, 255, 0), 2)\n",
    "\n",
    "            # Putting the class name\n",
    "            cv2.putText(img, f\"{class_name} {score}\", (x0, y0 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "\n",
    "            # If the class name is in the classes to blurr, blurr the image\n",
    "            if class_name in classes_to_blurr:\n",
    "                # Blurring the image\n",
    "                img[y0:y1, x0:x1] = cv2.blur(img[y0:y1, x0:x1], (30, 30))\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    # Defining the path to the image\n",
    "    image_path = os.path.join(postprocessed_images_dir_video, os.path.basename(image_path))\n",
    "\n",
    "    # If the image exists in the postprocessed images dir, remove it\n",
    "    if os.path.exists(image_path):\n",
    "        os.remove(image_path)\n",
    "\n",
    "    # Saving the image\n",
    "    cv2.imwrite(image_path, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a video from the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting all the images in the postprocessed images dir to a video\n",
    "images = os.listdir(postprocessed_images_dir_video)\n",
    "\n",
    "# Creating the image dictionary where the key is the image name and the image index \n",
    "image_dict = dict()\n",
    "for image in images:\n",
    "    image_dict[image] = int(image.split('.')[0])\n",
    "\n",
    "# Sorting the images by the index\n",
    "images = sorted(images, key=lambda x: image_dict[x])\n",
    "\n",
    "# Creating the output directory \n",
    "output_dir = os.path.join(os.getcwd(), 'output', video_name)\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Defining the output video path\n",
    "output_video_path = os.path.join(output_dir, 'output_video.mp4')\n",
    "\n",
    "# Defining the fps\n",
    "fps = 24\n",
    "\n",
    "# Defining the size of the video\n",
    "# Reading the first image to get the sizes \n",
    "img = cv2.imread(os.path.join(postprocessed_images_dir_video, images[0]))\n",
    "size = (img.shape[1], img.shape[0])\n",
    "\n",
    "# Defining the video writer\n",
    "out = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, size)\n",
    "\n",
    "# Iterating over the images\n",
    "for image in tqdm(images):\n",
    "    # Reading the image\n",
    "    img = cv2.imread(os.path.join(postprocessed_images_dir_video, image))\n",
    "\n",
    "    # Writing the image\n",
    "    out.write(img)\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Closing the video writer\n",
    "out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "waldo-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
